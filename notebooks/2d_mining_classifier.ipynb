{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import ee\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from shapely.geometry import box\n",
    "from geojson import Point, Feature, FeatureCollection, dump\n",
    "import geopandas\n",
    "\n",
    "sys.path.append('../')\n",
    "from scripts.get_s2_data_ee import get_history, get_history_polygon, get_pixel_vectors\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel 2 band descriptions\n",
    "band_descriptions = {\n",
    "    'B1': 'Aerosols, 442nm',\n",
    "    'B2': 'Blue, 492nm',\n",
    "    'B3': 'Green, 559nm',\n",
    "    'B4': 'Red, 665nm',\n",
    "    'B5': 'Red Edge 1, 704nm',\n",
    "    'B6': 'Red Edge 2, 739nm',\n",
    "    'B7': 'Red Edge 3, 779nm',\n",
    "    'B8': 'NIR, 833nm',\n",
    "    'B8A': 'Red Edge 4, 864nm',\n",
    "    'B9': 'Water Vapor, 943nm',\n",
    "    'B11': 'SWIR 1, 1610nm',\n",
    "    'B12': 'SWIR 2, 2186nm'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sampling Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..', '..', 'mining', 'data')\n",
    "\n",
    "with open(os.path.join(data_dir, 'MinesPos2018-2020Sentinel.geojson'), 'r') as f:\n",
    "    positive_sites = json.load(f)['features']\n",
    "    \n",
    "with open(os.path.join(data_dir, 'MinesNeg2018-2020Sentinel.geojson'), 'r') as f:\n",
    "    negative_sites = json.load(f)['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_sites = pd.DataFrame({\n",
    "    'name': ['positive_' + str(i) for i in range(len(positive_sites))],\n",
    "    'lon': [np.squeeze(site['geometry']['coordinates'])[0][0] for site in positive_sites],\n",
    "    'lat': [np.squeeze(site['geometry']['coordinates'])[0][1] for site in positive_sites],\n",
    "    'coords': [np.squeeze(site['geometry']['coordinates'])[0] for site in positive_sites],\n",
    "    'polygons': [ee.FeatureCollection([site]) for site in positive_sites]\n",
    "})\n",
    "display(mining_sites.head())\n",
    "\n",
    "jungle_sites = pd.DataFrame({\n",
    "    'name': ['negative_' + str(i) for i in range(len(negative_sites))],\n",
    "    'lon': [np.squeeze(site['geometry']['coordinates'])[0][0] for site in negative_sites],\n",
    "    'lat': [np.squeeze(site['geometry']['coordinates'])[0][1] for site in negative_sites],\n",
    "    'coords': [np.squeeze(site['geometry']['coordinates'])[0] for site in negative_sites],\n",
    "    'polygons': [ee.FeatureCollection([site]) for site in negative_sites]\n",
    "})\n",
    "display(jungle_sites.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'MinesNeg_caleb_selection.geojson'), 'r') as f:\n",
    "    caleb_negatives = json.load(f)['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_river_sites = pd.DataFrame({\n",
    "    'name': ['river_negative_' + str(i) for i in range(len(caleb_negatives))],\n",
    "    'lon': [np.squeeze(site['geometry']['coordinates'])[0] for site in caleb_negatives],\n",
    "    'lat': [np.squeeze(site['geometry']['coordinates'])[1] for site in caleb_negatives],\n",
    "    'coords': [np.squeeze(site['geometry']['coordinates']) for site in caleb_negatives],\n",
    "})\n",
    "display(negative_river_sites.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive site coordinates\n",
    "positive_coords = list(mining_sites['coords'])\n",
    "positive_names = list(mining_sites['name'])\n",
    "print(len(positive_coords), 'positive sites loaded')\n",
    "\n",
    "# Negative site coordinates\n",
    "negative_coords = list(pd.concat([jungle_sites['coords'], negative_river_sites['coords']]))\n",
    "negative_names = list(pd.concat([jungle_sites['name'], negative_river_sites['name']]))\n",
    "print(len(negative_coords), 'negative sites loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter rect width in degrees (0.035 max recommended) and site coordinates\n",
    "rect_width = 0.002\n",
    "num_months = 12\n",
    "start_date = '2018-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_history = get_history(positive_coords, \n",
    "                               positive_names, \n",
    "                               rect_width,\n",
    "                               num_months = num_months,\n",
    "                               start_date = start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_history = get_history(negative_coords, \n",
    "                               negative_names, \n",
    "                               rect_width,\n",
    "                               num_months = num_months,\n",
    "                               start_date = start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(negative_history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, f'2d_mining_sites_{start_date}_{num_months}.pkl'), 'wb') as f:\n",
    "    pickle.dump(positive_history, f)\n",
    "    \n",
    "with open(os.path.join(data_dir, f'2d_negative_sites_{start_date}_{num_months}.pkl'), 'wb') as f:\n",
    "    pickle.dump(negative_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_stack(patch_history):\n",
    "    img_stack = []\n",
    "    for date in patch_history:\n",
    "        for site in patch_history[date]:\n",
    "            spectral_stack = []\n",
    "            band_shapes = [np.shape(patch_history[date][site][band])[0] for band in band_descriptions]\n",
    "            if np.array(band_shapes).all() > 0:\n",
    "                for band in band_descriptions:\n",
    "                    spectral_stack.append(patch_history[date][site][band])\n",
    "                if np.median(spectral_stack) > 0:\n",
    "                    img_stack.append(np.rollaxis(np.array(spectral_stack), 0, 3))\n",
    "    return img_stack\n",
    "\n",
    "def normalize(x):\n",
    "    return (np.array(x)) / (3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_img = create_img_stack(positive_history)\n",
    "print(len(positive_img), 'positive images extracted')\n",
    "\n",
    "negative_img = create_img_stack(negative_history)\n",
    "print(len(negative_img), 'negative images extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_positives = []\n",
    "for img in positive_img:\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            subsampled_positives.append(img[i*28:(i+1)*28, \n",
    "                                            j*28:(j+1)*28, :])\n",
    "subsampled_negatives = []\n",
    "for img in negative_img:\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            subsampled_negatives.append(img[i*28:(i+1)*28, \n",
    "                                            j*28:(j+1)*28, :])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_img = np.array(subsampled_positives)\n",
    "negative_img = np.array(subsampled_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dimension = np.min([np.shape(img)[0] for img in positive_img])\n",
    "positive_img = [img[:min_dimension, :min_dimension, :] for img in positive_img]\n",
    "negative_img = [img[:min_dimension, :min_dimension, :] for img in negative_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = normalize(np.concatenate((positive_img, negative_img)))\n",
    "y = np.concatenate((np.ones(len(positive_img)), np.zeros(len(negative_img))))\n",
    "x, y = shuffle(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)\n",
    "print(\"Num Train:\\t\\t\", len(x_train))\n",
    "print(\"Num Test:\\t\\t\", len(x_test))\n",
    "print(f\"Percent Negative Train:\\t {100 * sum(y_train == 0.0) / len(y_train):.1f}\")\n",
    "print(f\"Percent Negative Test:\\t {100 * sum(y_test == 0.0) / len(y_test):.1f}\")\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.shape(x_train[0])\n",
    "print(\"Input Shape:\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(16, kernel_size=(3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(3)),\n",
    "        layers.Conv2D(32, kernel_size=(3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(3)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=360,\n",
    "    #brightness_range=[0.8,1.2],\n",
    "    width_shift_range=[0.8, 1.2],\n",
    "    height_shift_range=[0.8, 1.2],\n",
    "    #shear_range=10,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12), facecolor=(1,1,1))\n",
    "images, labels = datagen.flow(x_train, y_train, batch_size=36).next()\n",
    "for index, (image, label) in enumerate(zip(images, labels)):\n",
    "\n",
    "    rgb = np.stack((image[:,:,3],\n",
    "                    image[:,:,2], \n",
    "                    image[:,:,1]), axis=-1)\n",
    "    plt.subplot(6,6,index+1)\n",
    "    plt.imshow(np.clip(rgb, 0, 1))\n",
    "    if label[1] == 1:\n",
    "        plt.title('Mine')\n",
    "    else:\n",
    "        plt.title('No Mine')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Data Augmentation Examples')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size), \n",
    "          epochs=epochs, \n",
    "          validation_data = (x_test, y_test),\n",
    "          verbose = 1\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Network on Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/1-20-2021-filtered-both.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_stack(patch_history):\n",
    "    img_stack = []\n",
    "    for date in patch_history:\n",
    "        for site in patch_history[date]:\n",
    "            spectral_stack = []\n",
    "            band_shapes = [np.shape(patch_history[date][site][band])[0] for band in band_descriptions]\n",
    "            if np.array(band_shapes).all() > 0:\n",
    "                for band in band_descriptions:\n",
    "                    spectral_stack.append(patch_history[date][site][band])\n",
    "                if np.median(spectral_stack) > 0:\n",
    "                    img_stack.append(np.rollaxis(np.array(spectral_stack), 0, 3))\n",
    "    return img_stack\n",
    "\n",
    "def normalize(x):\n",
    "    return (np.array(x)) / (3000)\n",
    "\n",
    "def create_sampling_grid(min_lon, max_lon, min_lat, max_lat, steps_lon, steps_lat):\n",
    "    lons = np.linspace(min_lon, max_lon, steps_lon)\n",
    "    lats = np.linspace(min_lat, max_lat, steps_lat)\n",
    "    lon, lat = np.meshgrid(lons, lats)\n",
    "    coords = [[lon, lat] for lon, lat in zip(lon.flatten(), lat.flatten())]\n",
    "    return coords\n",
    "\n",
    "def plot_sampling_grid(coords, rect_width=0.025, output=False):\n",
    "    sampling_df = pd.DataFrame({\n",
    "        'lon': [coord[0] for coord in coords],\n",
    "        'lat': [coord[1] for coord in coords],\n",
    "        'pred': [0 for _ in range(len(coords))]\n",
    "    })\n",
    "    \n",
    "    features = []\n",
    "    for lon, lat in zip([coord[0] for coord in coords], [coord[1] for coord in coords]):\n",
    "        rect = box(lon - rect_width / 2, lat - rect_width / 2, lon + rect_width / 2, lat + rect_width / 2)\n",
    "        features.append(Feature(geometry=rect))\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    geopandas.GeoDataFrame.from_features(feature_collection).plot(figsize=(10, 8))\n",
    "    \n",
    "    if output:\n",
    "        return feature_collection\n",
    "    \n",
    "\n",
    "def get_image_stack(coords, start_date='2020-05-01', rect_width=0.025, scale=100):\n",
    "    names = ['sample_' + str(i) for i in range(len(coords))]\n",
    "    history = get_history(coords, \n",
    "                          names,\n",
    "                          rect_width,\n",
    "                          start_date=start_date,\n",
    "                          num_months=1,\n",
    "                          #scale=rect_width * (100 / 0.025)\n",
    "                          scale=scale\n",
    "                         )\n",
    "    img_stack = create_img_stack(history)\n",
    "    print(\"Image shape before cropping:\", img_stack[0].shape)\n",
    "    min_dim = np.min(img_stack[0].shape[:2])\n",
    "    img_stack = [img[:min_dim, :min_dim, :] for img in img_stack]\n",
    "    \n",
    "    return history, img_stack\n",
    "\n",
    "def predict_grid(model, history, img_stack, coords):\n",
    "    \n",
    "    preds = model.predict(normalize(img_stack))[:,1]\n",
    "    \n",
    "    cloud_free_coords = []\n",
    "    for site, coords in zip(history[start_date], coords):\n",
    "        if np.median(history[start_date][site]['B2']) > 0:\n",
    "            cloud_free_coords.append(coords)\n",
    "\n",
    "    preds_df = pd.DataFrame({\n",
    "        'pred': preds,\n",
    "        'lon': [coord[0] for coord in cloud_free_coords],\n",
    "        'lat': [coord[1] for coord in cloud_free_coords]}\n",
    "    )\n",
    "\n",
    "    return preds_df\n",
    "    \n",
    "def write_data(data_frame, file_path, rect_width):\n",
    "    data_frame.to_csv(file_path + '.csv', index=False)\n",
    "    \n",
    "    features = []\n",
    "    for lon, lat, pred in zip(list(data_frame['lon']), list(data_frame['lat']), list(data_frame['pred'])):\n",
    "        rect = box(lon - rect_width / 2, lat - rect_width / 2, lon + rect_width / 2, lat + rect_width / 2)\n",
    "        features.append(Feature(geometry=rect, properties={'pred': pred}))\n",
    "\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    with open(file_path + '.geojson', 'w') as f:\n",
    "       dump(feature_collection, f)\n",
    "    \n",
    "    geopandas.GeoDataFrame.from_features(feature_collection).plot(column='pred', \n",
    "                                                                  cmap='seismic',\n",
    "                                                                  figsize=(10, 8),\n",
    "                                                                  vmin=0,\n",
    "                                                                  vmax=1)\n",
    "\n",
    "    \n",
    "def stretch_histogram(array, min_val=0.1, max_val=0.75, gamma=1.2):\n",
    "    clipped = np.clip(array, min_val, max_val)\n",
    "    stretched = np.clip((clipped - min_val) / (max_val - min_val) ** gamma, 0, 1)\n",
    "    return stretched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rect_width = 0.02\n",
    "\n",
    "#tambopata whole\n",
    "min_lon, max_lon = -70.72, -69.8\n",
    "min_lat, max_lat = -13.2, -12.57\n",
    "\n",
    "# Caroni whole\n",
    "#min_lon, max_lon = -63.09674922312161, -62.18700279001181\n",
    "#min_lat, max_lat = 4.514923184841662, 6.511625537541098\n",
    "\n",
    "#min_lon, max_lon = -70.64, -70.4\n",
    "#min_lat, max_lat = -12.96, -13.07\n",
    "\n",
    "\n",
    "steps_lon = 20\n",
    "steps_lat = 15\n",
    "\n",
    "sampling_coords = create_sampling_grid(min_lon, max_lon, min_lat, max_lat, steps_lon, steps_lat)\n",
    "plot_sampling_grid(sampling_coords, rect_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date='2019-05-01'\n",
    "scale = 20\n",
    "history, img_stack = get_image_stack(sampling_coords, start_date=start_date, rect_width=rect_width, scale=scale)\n",
    "\n",
    "file_path = f\"../data/tambopata_mine_{min_lon},{max_lon}_{min_lat},{max_lat}_{rect_width}_{scale}\"\n",
    "with open(file_path + '.pkl', 'wb') as f:\n",
    "    pickle.dump(img_stack, f)\n",
    "\n",
    "preds_df = predict_grid(model, history, img_stack, sampling_coords)\n",
    "write_data(preds_df, file_path, rect_width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date='2019-09-01'\n",
    "scale = 20\n",
    "history, img_stack = get_image_stack(positive_coords, start_date=start_date, rect_width=rect_width, scale=scale)\n",
    "file_path = f\"../data/positive_sites_{rect_width}_{scale}\"\n",
    "with open(file_path + '.pkl', 'wb') as f:\n",
    "    pickle.dump(img_stack, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date='2019-09-01'\n",
    "scale = 20\n",
    "history, img_stack = get_image_stack(negative_coords, start_date=start_date, rect_width=rect_width, scale=scale)\n",
    "\n",
    "file_path = f\"../data/negative_sites_{rect_width}_{scale}\"\n",
    "with open(file_path + '.pkl', 'wb') as f:\n",
    "    pickle.dump(img_stack, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_stack[0][:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = int(np.ceil(np.sqrt(len(img_stack))))\n",
    "\n",
    "plt.figure(figsize=(36,36), dpi=150)\n",
    "for index, (img, pred) in enumerate(zip(img_stack, preds_df['pred'])):\n",
    "    plt.subplot(num_img, num_img, index + 1)\n",
    "    plt.imshow(stretch_histogram(normalize(np.stack((img[:,:,3],\n",
    "                         img[:,:,2],\n",
    "                         img[:,:,1]), axis=-1\n",
    "                        ))))\n",
    "    plt.title(f\"{pred:.0%}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network on candidate site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_months = 6\n",
    "test_coords = [-62.42462292953776, 4.870750832241027]\n",
    "#test_coords = [-62.39958492247051, 4.848350836417974]\n",
    "#test_coords = [-61.73903184056069, 4.3381195947723405]\n",
    "#test_coords = [-61.74202162706261, 4.417347798525773]\n",
    "test_patch = get_history([test_coords], \n",
    "                           ['test_patch'],\n",
    "                           rect_width,\n",
    "                           start_date=start_date,\n",
    "                           num_months=num_months)\n",
    "\n",
    "test_data = create_img_stack(test_patch)\n",
    "test_data = [img[:84, :84, :] for img in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = int(np.ceil(np.sqrt(len(test_data))))\n",
    "preds = []\n",
    "plt.figure(figsize=(8,8), facecolor=(1,1,1))\n",
    "for index, month in enumerate(test_data):\n",
    "    im = np.expand_dims(normalize(month), 0)\n",
    "    rgb = np.stack((normalize(month[:,:,3]),\n",
    "                    normalize(month[:,:,2]), \n",
    "                    normalize(month[:,:,1])), axis=-1)\n",
    "    pred = model.predict(im)[0][1]\n",
    "    preds.append(pred)\n",
    "    plt.subplot(num_img, num_img, index + 1)\n",
    "    plt.imshow(np.clip(rgb, 0, 1), vmin=0, vmax=1)\n",
    "    plt.title(f\"{100 * pred:.0f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle(f\"Mean Classification: {100*np.mean(preds):.0f}%\\nSite: {test_coords[1]:.3f}°, {test_coords[0]:.3f}°\", size=16)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f'/Users/ckruse/Downloads/{test_coords[1]:32f}, {test_coords[0]:.3f}.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network on river sampling sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..', '..', 'mining', 'data')\n",
    "\n",
    "with open(os.path.join(data_dir, 'test_sampling_sites.geojson'), 'r') as f:\n",
    "    test_sites = json.load(f)['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sites = pd.DataFrame({\n",
    "    'name': ['test_' + str(i) for i in range(len(test_sites[0]['geometry']['coordinates']))],\n",
    "    'lon': [coords[0] for coords in test_sites[0]['geometry']['coordinates']],\n",
    "    'lat': [coords[0] for coords in test_sites[0]['geometry']['coordinates']],\n",
    "    'coords': [coords[:2] for coords in test_sites[0]['geometry']['coordinates']],\n",
    "})\n",
    "display(test_sites.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0.005\n",
    "sampling_coords = []\n",
    "sampling_names = []\n",
    "for coord, name in zip(test_sites['coords'], test_sites['name']):\n",
    "    sampling_coords.append(coord)\n",
    "    sampling_coords.append([coord[0] - offset, coord[1] - offset])\n",
    "    sampling_coords.append([coord[0] + offset, coord[1] + offset])\n",
    "    \n",
    "    sampling_names.append(name)\n",
    "    sampling_names.append(name + '_right')\n",
    "    sampling_names.append(name + '_left')\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter([coord[0] for coord in sampling_coords], [coord[1] for coord in sampling_coords], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([coord[0] for coord in sampling_coords], [coord[1] for coord in sampling_coords])\n",
    "a = pd.DataFrame()\n",
    "a['lat'] = [coord[1] for coord in sampling_coords]\n",
    "a['lon'] = [coord[0] for coord in sampling_coords]\n",
    "a.to_csv('/Users/ckruse/Downloads/river_sampling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_sampling = get_history(sampling_coords, \n",
    "                         sampling_names,\n",
    "                         0.0025,\n",
    "                         start_date=start_date,\n",
    "                         num_months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_sampling[start_date]['test_0']['B2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_patches = get_history(test_sites['coords'], \n",
    "                         test_sites['name'],\n",
    "                         rect_width,\n",
    "                         start_date=start_date,\n",
    "                         num_months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_data = create_img_stack(river_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_sampling_data = create_img_stack(river_sampling)\n",
    "river_sampling_data = [img[:28, :28, :] for img in river_sampling_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_data = [img[:min_dimension, :min_dimension, :] for img in river_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(river_sampling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_river = []\n",
    "subsampled_coords = []\n",
    "for img, coords in zip(river_sampling_data, sampling_coords):\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            subsampled_lon = coords[0] + rect_width * ((0.5 * j) - 1)\n",
    "            subsampled_lat = coords[1] - rect_width * ((0.5 * i) - 1)\n",
    "            subsampled_coords.append([subsampled_lon, subsampled_lat])\n",
    "            subsampled_river.append(img[i*28:(i+1)*28, j*28:(j+1)*28, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(subsampled_river[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in river_sampling[start_date]:\n",
    "    if np.min(river_sampling[start_date][site]['B2']) < 0:\n",
    "          print(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(np.array(sampling_names) == 'test_54_right'))\n",
    "print(np.where(np.array(sampling_names) == 'test_55_right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sampling_names).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_coords[163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((sampling_coords[:163], sampling_coords[164:166], sampling_coords[167:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(normalize(river_sampling_data))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(preds, columns=['pred'])\n",
    "predictions['lat'] = [coord[1] for coord in np.concatenate((sampling_coords[:163], sampling_coords[164:166], sampling_coords[167:]))]\n",
    "predictions['lon'] = [coord[0] for coord in np.concatenate((sampling_coords[:163], sampling_coords[164:166], sampling_coords[167:]))]\n",
    "predictions.head()\n",
    "predictions.to_csv('/Users/ckruse/Downloads/subsampled_mine_preds_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot blob locations on a satellite base image\n",
    "from keplergl import KeplerGl\n",
    "river_map = KeplerGl(data={'samples': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_samples = 20 ** 2\n",
    "num_img = int(np.ceil(np.sqrt(len(river_sampling_data[:num_samples]))))\n",
    "preds = []\n",
    "plt.figure(figsize=(24,24), facecolor=(1,1,1))\n",
    "for index, site in enumerate(river_sampling_data[:num_samples]):\n",
    "    im = np.expand_dims(normalize(site), 0)\n",
    "    rgb = np.stack((normalize(site[:,:,3]),\n",
    "                    normalize(site[:,:,2]), \n",
    "                    normalize(site[:,:,1])), axis=-1)\n",
    "    pred = model.predict(im)[0][1]\n",
    "    preds.append(pred)\n",
    "    plt.subplot(num_img, num_img, index + 1)\n",
    "    plt.imshow(np.clip(rgb, 0, 1), vmin=0, vmax=1)\n",
    "    plt.title(f\"{100 * pred:.0f}%\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f'/Users/ckruse/Downloads/{test_coords[1]:32f}, {test_coords[0]:.3f}.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tambopata Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of coordinates:\n",
    "min_lon, max_lon = (-70.72, -69.8)\n",
    "min_lat, max_lat = (-13.07, -12.73)\n",
    "\n",
    "num_samples = 30\n",
    "\n",
    "lons = np.linspace(min_lon, max_lon, num_samples)\n",
    "lats = np.linspace(min_lat, max_lat, num_samples)\n",
    "lon, lat = np.meshgrid(lons, lats)\n",
    "peru_coords = [[lon, lat] for lon, lat in zip(lon.flatten(), lat.flatten())]\n",
    "peru_names = ['tambopata_' + str(i) for i in range(len(peru_coords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = '2020-05-01'\n",
    "tambopata = get_history(peru_coords, \n",
    "                        peru_names,\n",
    "                        0.0025,\n",
    "                        start_date=start_date,\n",
    "                        num_months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tambopata_stack = create_img_stack(tambopata)\n",
    "tambopata_stack = [img[:28, :28, :] for img in tambopata_stack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(normalize(tambopata_stack))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_free_coords = []\n",
    "for site, coords in zip(tambopata[start_date], peru_coords):\n",
    "    if np.min(tambopata[start_date][site]['B2']) > 0:\n",
    "        cloud_free_coords.append(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peru_preds = pd.DataFrame(preds, columns=['pred'])\n",
    "peru_preds['lat'] = [coord[1] for coord in cloud_free_coords]\n",
    "peru_preds['lon'] = [coord[0] for coord in cloud_free_coords]\n",
    "\n",
    "peru_preds.to_csv('/Users/ckruse/Downloads/tambopata_preds.csv')\n",
    "peru_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_width = 0.025\n",
    "min_lon, max_lon = -70.72, -69.8\n",
    "min_lat, max_lat = -13.2, -12.57\n",
    "num_samples = 20\n",
    "sampling_grid = create_sampling_grid(min_lon, max_lon, min_lat, max_lat, num_samples, num_samples)\n",
    "sampling_df = pd.DataFrame({\n",
    "    'lon': [coord[0] for coord in sampling_grid],\n",
    "    'lat': [coord[1] for coord in sampling_grid],\n",
    "    'pred': [0 for _ in range(len(sampling_grid))]\n",
    "})\n",
    "\n",
    "write_data(sampling_df, f'/Users/ckruse/Downloads/grid_{min_lon},{max_lon}_{min_lat},{max_lat}_{rect_width}', rect_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tambopata_train_df, tambopata_train_img = predict_grid(min_lon, max_lon, min_lat, max_lat, num_samples, num_samples, rect_width=rect_width)\n",
    "write_data(tambopata_train_df, f'/Users/ckruse/Downloads/tambopata_{min_lon},{max_lon}_{min_lat},{max_lat}_{rect_width}', rect_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peru_south_wide = predict_grid(-70.72, -68.88, -13.41, -13.07, 60, 30)\n",
    "peru_south_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([peru_preds, peru_preds_north, peru_east, peru_south, peru_south_wide]).to_csv(os.path.join(data_dir, 'tambopata_grid_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of coordinates:\n",
    "min_lon, max_lon = (-70.72, -69.8)\n",
    "min_lat, max_lat = (-13.07, -12.73)\n",
    "\n",
    "num_samples = 30\n",
    "\n",
    "lons = np.linspace(min_lon, max_lon, num_samples)\n",
    "lats = np.linspace(min_lat, max_lat, num_samples)\n",
    "lon, lat = np.meshgrid(lons, lats)\n",
    "peru_coords = [[lon, lat] for lon, lat in zip(lon.flatten(), lat.flatten())]\n",
    "peru_names = ['tambopata_' + str(i) for i in range(len(coords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = '2020-05-01'\n",
    "tambopata_north = get_history(peru_coords, \n",
    "                        peru_names,\n",
    "                        0.0025,\n",
    "                        start_date=start_date,\n",
    "                        num_months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tambopata_stack_north = create_img_stack(tambopata_north)\n",
    "tambopata_stack_north = [img[:28, :28, :] for img in tambopata_stack_north]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tambopata_stack_north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_north = model.predict(normalize(tambopata_stack_north))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_free_coords_north = []\n",
    "for site, coords in zip(tambopata_north[start_date], peru_coords):\n",
    "    if np.min(tambopata_north[start_date][site]['B2']) > 0:\n",
    "        cloud_free_coords_north.append(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peru_preds_north = pd.DataFrame(preds_north, columns=['pred'])\n",
    "peru_preds_north['lat'] = [coord[1] for coord in cloud_free_coords_north]\n",
    "peru_preds_north['lon'] = [coord[0] for coord in cloud_free_coords_north]\n",
    "\n",
    "peru_preds_north.to_csv('/Users/ckruse/Downloads/tambopata_preds_north.csv')\n",
    "peru_preds_north.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([peru_preds, peru_preds_north, peru_east, peru_south]).to_csv(os.path.join(data_dir, 'tambopata_grid.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## River Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tambopata_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_head.to_csv('/Users/ckruse/Downloads/river_head.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tambopata_train_df, tambopata_train_img = predict_grid(-70.6, -70.3, -13.01, -12.95, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(tambopata_train_df, '/Users/ckruse/Downloads/test_bigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
